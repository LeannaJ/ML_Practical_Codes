{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWppJEv2ERVQ"
   },
   "source": [
    "# Long Short-term Memory for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4RBxDv-ERVS"
   },
   "source": [
    "This notebook uses LSTM neural network to generate text from Nietzsche's writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vC0jdj6RERVS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2W9-aTzERVU"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL8x1DjdERVU"
   },
   "source": [
    "### Get the data\n",
    "Nietzsche's writing dataset is available online. The following code download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "06QxXBZDERVV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECFejNckERVV"
   },
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "JY47xw7-ERVV",
    "outputId": "e023abc7-294c-4b29-db79-e85c3997c765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "8Gwwa5wfERVW",
    "outputId": "40e2c5cf-5108-4544-df73-e1a117582cc6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supposing that truth is a woman--what then? is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to truth, have been unskilled and unseemly methods for\n",
      "winning a woman? certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--if,\n",
      "indeed, it stands at all!\n"
     ]
    }
   ],
   "source": [
    "print(text[10:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "LPI4SkywERVX",
    "outputId": "3b41be80-263d-4834-8274-b5517274545d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "# total nomber of characters\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ibV0CydERVX"
   },
   "source": [
    "### Clean data\n",
    "\n",
    "We cut the text in sequences of maxlen characters with a jump size of 3.\n",
    "The features for each example is a matrix of size maxlen*num of chars.\n",
    "The label for each example is a vector of size num of chars, which represents the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "R-jkujlsERVX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create (character, index) and (index, character) dictionary\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "HEBUyXyBERVY",
    "outputId": "96edd431-463d-461a-f0bc-d6686c47a6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200285\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FGg1ShmWmSD"
   },
   "source": [
    "### Vecterization **This vectorization procedure includes a one-hot-encoding for each word, and therefore, an Embedding layer should not be used. If you want to use an Embedding layer. You will need to revise this block. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Ftcdv7S4ERVY",
    "outputId": "02d6ba11-e84d-4584-de33-f692944646fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g04nzFKERVY"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVWhFLcbERVZ"
   },
   "source": [
    "### Build the model - fill in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRnIV0JtERVZ"
   },
   "source": [
    "we need a recurrent layer with input shape (maxlen, len(chars)) and a dense layer with output size  len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "fzSVtBghERVZ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200285, 40, 57)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(256, return_sequences=True, input_shape=(maxlen, len(chars)))) \n",
    "model.add(layers.LSTM(256)) \n",
    "model.add(layers.Dense(len(chars), activation='softmax'))  \n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',  \n",
    "              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xte4grhqERVZ"
   },
   "source": [
    "### Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "xZYsX4p0ERVZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">321,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,649</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m321,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)             │        \u001b[38;5;34m14,649\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">861,497</span> (3.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m861,497\u001b[0m (3.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">861,497</span> (3.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m861,497\u001b[0m (3.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9q9AlZnERVa"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "QDLnDNk7ERVa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "LSXssBV3ERVa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrintLoss(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, _):\n",
    "        # Function invoked at end of each epoch. Prints generated text.\n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        for diversity in [0.5, 1.0]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "A-KBDhU0ERVa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 2.7239\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" but of a very limited kind in compariso\"\n",
      " but of a very limited kind in comparisouled the mong the lought the rething prestion of the the will the most the expinate the cooprest in sidsouss, and werther for ever to it is as sould the selpess and and for it is the loving the porpoustiont of the gore is suthing at indous of the serbent and peremon aly centerstion of the conced in the sent the grees and esting it the soplestenter of the aster in the prising the per of perate to p\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" but of a very limited kind in compariso\"\n",
      " but of a very limited kind in compariso bither,\n",
      "ond pracianter--verent of zevee and mesentaund, a illoots, me.\n",
      "\n",
      "259 giss it ( p lnoud is thelescel.\n",
      "\n",
      "92 thet ere\n",
      "tonk in smins muthigh- fetun\n",
      "bsounianmer. \"vury coms., ane an one mofe sis peitated no- to calqaocco his mentsentaen misn=ler to itser?\n",
      "stamet: sther thosen patt pot. ver as a soud casterseny rat duct\n",
      "phisepemranelloken\n",
      "tomny:e asisscinien of thelliyt by pan ever of the mints a\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 149ms/step - loss: 2.7236 - val_loss: 2.0257\n",
      "Epoch 2/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.9721\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"d religious formula with a new and deepe\"\n",
      "d religious formula with a new and deepence the love of the langeness that and it be sempration of which the prisentunce of the gronation, and man what the constion of the mand which fand the incernation of the ctrant the which now not herlowing of the proped of the herst of the moraling even the the belision the hersenty which the light is as the streed soul of the which him the his a men the freation of the rapent the supperiation of \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"d religious formula with a new and deepe\"\n",
      "d religious formula with a new and deepely\"n of made with a\n",
      "esficatatious. in what the without but of\n",
      "lecernased equating of us now kpinstandimucy thilg? haw the sene incouthents\n",
      "to the is reantion, often of whomer. as one homent only\n",
      "thoughter this musfertie\n",
      "this dongeal call anlaytion, in\n",
      "a dengide\n",
      "which his\n",
      "ingod mures, than the inmence horly. we\" the deaision utill the sisidepe hims thill nech!s--our heinided\n",
      "well compreany afleangs\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 152ms/step - loss: 1.9720 - val_loss: 1.8064\n",
      "Epoch 3/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.7407\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t those who refused him their love; the \"\n",
      "t those who refused him their love; the man a stranger--and not in such a same of the realing and the suppersion the something and presided and which chered the indisting of their suncisticl the workan\n",
      "believes the profounce that is asterthard by a self-were, and and becausing\n",
      "supcessical freederss of subite and general that is sict of the presenting to be the enderses this spirit of the conscience that the more and are sunterthing indi\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t those who refused him their love; the \"\n",
      "t those who refused him their love; the will by thereby\n",
      "conscecterts of sladds, how simpinisuly are and firith.; have of tist in conction of \"umpacusely a suclears\n",
      "undes to hhough om oncemsed theropodic abdaccfially windsms about of maning, but notrdanst, meraphyser--and they for accapually\n",
      "sciodly authothers of inseally. so _pormen by the\n",
      "repion life a septiscience sign to beyt navests, no lo gheist in will becaised and neare which wem\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 153ms/step - loss: 1.7407 - val_loss: 1.6767\n",
      "Epoch 4/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1.5875\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"her, more extended, more\n",
      "comprehensive s\"\n",
      "her, more extended, more\n",
      "comprehensive sympathy and supposition of interpous of something such a philosophers is the dast also the form of a good find even the sans and life with the soul of the conception of the fact\n",
      "for its to the may not to possition. such precession to the developed to sentains of the sertation of the morals and masters and such an a tould suck of the concepteres of the stringly in still to has consciences and and f\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"her, more extended, more\n",
      "comprehensive s\"\n",
      "her, more extended, more\n",
      "comprehensive strlegarable very picu mootharys soulistres jeyging tooking tywallsg sense is datk of fact, i as it as to tyempome by\n",
      "that\n",
      "viewens shorthisped; and dubourselves calkn thereing still, who have doathers: liftre hivaling it, which far\n",
      "hereing and of beemen seesion of infuction--hered yousting somethingly even probours, thus cexticco sevoreven picturial cloigrat viritaring that the seruenly it would si\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 155ms/step - loss: 1.5875 - val_loss: 1.6007\n",
      "Epoch 5/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 1.4773\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" sprightly teutonomania for i have alrea\"\n",
      " sprightly teutonomania for i have already the extent of the desire of the most perhaps it is which is here not believe only the power of the privilege and still in the finer so the significe of the present of self-ressenting the say to its mare from instances of have manifully in the form the tomen his will to him hand as it was not as so from has at a philosophy of the profound for a some and feniarity of the strong of the sensious an\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" sprightly teutonomania for i have alrea\"\n",
      " sprightly teutonomania for i have alreaty in the care of spirit respently\n",
      "the known the \"internies equerce,\n",
      "nepusious of which syntation\n",
      "of sencialness ferers unascos. the teme chremped extens?1\n",
      "\n",
      "40] many all reconferible\n",
      "for than men: hitherto paty. we is for\n",
      "sould for every, what it knowledse, it as but is cussed and prous reverence time the pestical relagion\n",
      "intedlomanific formary.] when it was viltionation as\n",
      "hellogical dome conseq\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 157ms/step - loss: 1.4773 - val_loss: 1.5507\n",
      "Epoch 6/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 1.3956\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nd become so thin and emaciated, that at\"\n",
      "nd become so thin and emaciated, that at present to the condition of the man dream, conternity of the present instinctive last and and and of this stronged that is the greatest and with an actions all the subpression and painful\n",
      "han and eviduble very some the senses afford of distrustion, and well experiences and popular problem earth of a supposition of a septics of the spirits and distartific to him world in their present of such as d\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nd become so thin and emaciated, that at\"\n",
      "nd become so thin and emaciated, that at does could at least, or \"make farce, to self the aspetiialness and ary, who harevel calles leass, a perforcal personough, more\n",
      "defeir entipate fain secief of an prevelouman\n",
      "ferming, and displession\n",
      "in the active casses of viltion of interpretated,\n",
      "that in a nowlesge philistence in his stupidity) and perplassial, and time in\n",
      "liftle. was should eppomate per\n",
      "art to not, hholess see reap agons where\n",
      "\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 157ms/step - loss: 1.3956 - val_loss: 1.5341\n",
      "Epoch 7/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 1.3097\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y of the celebrated conception of \"free\n",
      "\"\n",
      "y of the celebrated conception of \"free\n",
      "spirits, can a some doint to express\n",
      "recossible the demostion of an influence of\n",
      "philosophers, and commont the most procuse that is as a common of all form of such of the than a \"last repogulity.\n",
      "\n",
      "299. the such and always the recognizence of the spiritual mankind with a might be the unmoration that is a complementage of the instinctively and and corror of the former of the spiritual midding in the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y of the celebrated conception of \"free\n",
      "\"\n",
      "y of the celebrated conception of \"free\n",
      "spirit, should be knowledge if others he luforal, to our gor with injortable is encertial bang his proscipling a like time, had bounther, with and\n",
      "fine of\n",
      "life of \"laspuable\n",
      "itself adoundificed\n",
      "itself be azaig, but\n",
      "hand most\n",
      "absuratively and\n",
      "with moral utrecturness!\" it is also,\n",
      "a men upon aute is only grast lus: thought it is\n",
      "climatelly \"begie\" no longer only is the endly phinomoonchistress\n",
      "and\n",
      "l\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 157ms/step - loss: 1.3097 - val_loss: 1.5184\n",
      "Epoch 8/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.2439\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"this quick comprehension people always\n",
      "u\"\n",
      "this quick comprehension people always\n",
      "understand it is all the philosophy of the abort this philosophy of\n",
      "the belief in the most developitation of the world, there is do not in the world of the scientific secret of the lover of the world--not in the sitran man of the last refined and most discively and the strengness of stupidity. and finally and interpounting and about the same of everything that it is the truth\" and the love of the s\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"this quick comprehension people always\n",
      "u\"\n",
      "this quick comprehension people always\n",
      "uncalnately questioning: or\n",
      "dater, in its, dore. threater will in short idouth which the wrong is sonsthing; arver in which stame tide\n",
      "appears to their origin to which prouded and preasomy to hill cupting a\n",
      "knoung devolutors, now this type them, and stronge,\n",
      "what the memstany of \"wondelf on the us. as\n",
      "for it the slive phwicelly--it was indignately,\" is litten est to mudious for \"formeloum in mary o\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 154ms/step - loss: 1.2439 - val_loss: 1.5355\n",
      "Epoch 9/10\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1.1781\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t she believes\n",
      "of the eternally masculin\"\n",
      "t she believes\n",
      "of the eternally masculine man when they were such a surprised the belief in a moral as the man grown instinct of the corrusis of the lacking of nations, in short of the conscience which man have at all the sign of the more especially when the portent of present and sacrifice morality intentions are place in the problem of the internign of the individuals are\n",
      "and as the subject, profound man nor aftermanted to an artifies\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t she believes\n",
      "of the eternally masculin\"\n",
      "t she believes\n",
      "of the eternally masculine stampless\n",
      "and\n",
      "arbmasterly whet is rejucted will to man\" perhaps have\n",
      "more as figurator to ourselvation is\n",
      "becauses is induce to that it perhaps france all, for it as a man has been\n",
      "still life, we, frany charman tas from them. the ancervation of race impossible\n",
      "furm on\n",
      "eudope, our evolve\" each is delichther\n",
      "that perhaps because it belongs to their chaust of decerving in shadest and sintently over\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 159ms/step - loss: 1.1781 - val_loss: 1.5499\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH = 128\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x, y,\n",
    "                    batch_size = BATCH,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose = 1,\n",
    "                    callbacks = [early_stop, PrintLoss()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
